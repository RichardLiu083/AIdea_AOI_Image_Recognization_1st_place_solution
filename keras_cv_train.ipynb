{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.layers import Dense,Activation,Flatten,Conv2D,MaxPooling2D,Dropout, GlobalAvgPool2D, Softmax\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import losses\n",
    "import keras\n",
    "import efficientnet.keras as efn\n",
    "import xgboost as xgb\n",
    "import albumentations\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "seed= 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tensorflow.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow\n",
    "import gc\n",
    "\n",
    "# Reset Keras Session\n",
    "def reset_keras():\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del classifier # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tensorflow.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    set_session(tensorflow.Session(config=config))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet_v2 import ResNet50V2\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "\n",
    "def resnet(lr, label_smooth):\n",
    "    net= ResNet50V2(include_top=False, weights='imagenet', input_tensor=None,\n",
    "               input_shape=(None,None,3))\n",
    "    \n",
    "    model= Sequential()\n",
    "    model.add(net)\n",
    "    model.add(GlobalAvgPool2D())\n",
    "    model.add(Dense(6, activation= 'softmax'))\n",
    "    \n",
    "    model.layers[0].trainable= True\n",
    "    opt= keras.optimizers.Adam(learning_rate= lr)\n",
    "    loss= losses.CategoricalCrossentropy(label_smoothing= label_smooth, name= 'categorical_crossentropy' )\n",
    "    model.compile(loss=loss, optimizer= opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def effnet(lr, label_smooth):\n",
    "    \n",
    "    net= efn.EfficientNetB2(include_top=False, weights='imagenet', input_tensor=None,\n",
    "               input_shape=(None,None,3))\n",
    "    \n",
    "    model= Sequential()\n",
    "    model.add(net)\n",
    "    model.add(GlobalAvgPool2D())\n",
    "    model.add(Dense(6, activation= 'softmax'))\n",
    "    \n",
    "    model.layers[0].trainable= True\n",
    "    opt = keras.optimizers.Adam(learning_rate= lr)\n",
    "    loss= losses.CategoricalCrossentropy(label_smoothing= label_smooth, name= 'categorical_crossentropy' )\n",
    "    model.compile(loss=loss, optimizer= opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def xception(lr, label_smooth):\n",
    "    \n",
    "    net= Xception(include_top=False, weights='imagenet', input_tensor=None,\n",
    "               input_shape=(None,None,3))\n",
    "    \n",
    "    model= Sequential()\n",
    "    model.add(net)\n",
    "    model.add(GlobalAvgPool2D())\n",
    "    model.add(Dense(6, activation= None))\n",
    "    model.add(Softmax())\n",
    "    \n",
    "    model.layers[0].trainable= True\n",
    "    opt = keras.optimizers.Adam(learning_rate= lr)\n",
    "    loss= losses.CategoricalCrossentropy(label_smoothing= label_smooth, name= 'categorical_crossentropy' )\n",
    "    model.compile(loss=loss, optimizer= opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(train_x, train_y):\n",
    "    \n",
    "    new_x= []\n",
    "    new_y= []\n",
    "    \n",
    "    transform = albumentations.Compose([\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.Blur(blur_limit=2, p=0.5),\n",
    "        albumentations.ChannelShuffle(p=0.5),\n",
    "        #albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30,\n",
    "                                        interpolation=cv2.INTER_LINEAR, border_mode=2, p=0.5),\n",
    "    ])\n",
    "    \n",
    "    for i in range(len(train_x)):\n",
    "        \n",
    "        aug_1= transform(image= train_x[i].astype(np.float32))\n",
    "        new_x.append(aug_1['image'])\n",
    "        new_y.append(train_y[i])\n",
    "        \n",
    "        \n",
    "    return np.array(new_x), np.array(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(train_x, train_y, alpha= 0.2, prob= 0.5):\n",
    "    \n",
    "    new_x= []\n",
    "    new_y= []\n",
    "    \n",
    "    for i in range(train_x.shape[0]):\n",
    "        \n",
    "        if np.random.rand() <= prob: \n",
    "            new_x.append(train_x[i])\n",
    "            new_y.append(train_y[i])\n",
    "            continue\n",
    "        \n",
    "        while True:\n",
    "            fuse_index= np.random.randint(len(train_x))\n",
    "            if train_y[fuse_index].argmax(0)!= train_y[i].argmax(0): break\n",
    "            \n",
    "        mult= np.random.beta(alpha,alpha)\n",
    "        \n",
    "        mix_x= train_x[i]*mult + train_x[fuse_index]*(1-mult)\n",
    "        mix_y= train_y[i]*mult + train_y[fuse_index]*(1-mult)\n",
    "        \n",
    "        new_x.append(mix_x)\n",
    "        new_y.append(mix_y)\n",
    "        \n",
    "        \n",
    "    return np.array(new_x), np.array(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_folder, train_or_test, img_size):\n",
    "    \n",
    "    if train_or_test=='train':\n",
    "    \n",
    "        folder= os.listdir(img_folder)\n",
    "        train_x= []\n",
    "        train_y= []\n",
    "\n",
    "        \n",
    "        all_img_name= []\n",
    "        for f in folder:\n",
    "            \n",
    "            folder_img_name= os.listdir(img_folder+'/'+f)\n",
    "            all_img_name+= folder_img_name\n",
    "            \n",
    "            for name in folder_img_name:\n",
    "\n",
    "                img= cv2.imread(img_folder+'/'+f+'/'+name, 1)\n",
    "                img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)/255\n",
    "                img= cv2.resize(img, (img_size,img_size))\n",
    "                train_x.append(img)\n",
    "                train_y.append(folder.index(f))\n",
    "                \n",
    "                \n",
    "    else:\n",
    "        train_x= []\n",
    "        train_y= []\n",
    "        all_img_name= os.listdir(img_folder)\n",
    "        \n",
    "        for name in all_img_name:\n",
    "            img= cv2.imread(img_folder+'/'+name, 1)\n",
    "            img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)/255\n",
    "            img= cv2.resize(img, (img_size,img_size))\n",
    "            train_x.append(img)\n",
    "        \n",
    "        \n",
    "    return np.array(train_x), np.array(train_y), np.array(all_img_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG= {\n",
    "    'model_name': 'effnet',\n",
    "    'img_size': 128,\n",
    "    'batch_size': 20,\n",
    "    'epoch': 30,\n",
    "    'n_fold': 5,\n",
    "    'lr': 3e-4,\n",
    "    'label_smooth': 0.5,\n",
    "    'load_model': ''#'test_cv_model/xcepnet_8.29',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x, all_y, img_name= load_data('Dataset/train_dataset_expand', 'train', CFG['img_size'])\n",
    "all_y= to_categorical(all_y, 6)\n",
    "index= list(range(len(all_x)))\n",
    "np.random.shuffle(index)\n",
    "all_x= all_x[index]\n",
    "all_y= all_y[index]\n",
    "\n",
    "print(all_x.shape)\n",
    "print(all_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits= CFG['n_fold'], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cv, (train_index, vali_index) in enumerate(kf.split(all_x, all_y.argmax(1))):\n",
    "    #if cv!=0: continue\n",
    "    print('start cv: ', cv)\n",
    "\n",
    "    vali_x, vali_y= all_x[vali_index], all_y[vali_index]\n",
    "    epoch= CFG['epoch']\n",
    "    \n",
    "    \n",
    "    if CFG['load_model']!='':\n",
    "        print('load_model')\n",
    "        model= load_model(CFG['load_model']+'/'+'model_cv_'+str(cv)+'.h5')\n",
    "    else:\n",
    "        model= effnet(CFG['lr'], CFG['label_smooth'])\n",
    "\n",
    "        \n",
    "    best_vali_acc= 0\n",
    "    for ep in range(epoch):\n",
    "\n",
    "        aug_x, aug_y= all_x[train_index], all_y[train_index]\n",
    "        aug_x, aug_y= data_aug(aug_x, aug_y)\n",
    "        #aug_x, aug_y= mixup(aug_x, aug_y, 0.2)\n",
    "\n",
    "\n",
    "        y_integers = np.argmax(aug_y, axis=1)\n",
    "        class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "        d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        train_history= model.fit(x= aug_x, y= aug_y, validation_data=(vali_x, vali_y), epochs=1, \n",
    "                                 batch_size= CFG['batch_size'],\n",
    "                                 class_weight= d_class_weights,\n",
    "                                 verbose=1)\n",
    "\n",
    "\n",
    "        if train_history.history['val_accuracy'][0] > best_vali_acc:\n",
    "            best_vali_acc= train_history.history['val_accuracy'][0]\n",
    "            print('model save at vali_acc: ', train_history.history['val_accuracy'][0])\n",
    "            model.save('train_cv_model/model_cv_'+str(cv)+'.h5')\n",
    "\n",
    "        del aug_x\n",
    "        del aug_y\n",
    "        \n",
    "        if ep == epoch-15:\n",
    "            model= load_model('train_cv_model/model_cv_'+str(cv)+'.h5')\n",
    "            lr= CFG['lr']*0.1\n",
    "            print('reduce lr: ', lr)\n",
    "            K.set_value(model.optimizer.learning_rate, lr)\n",
    "        \n",
    "    reset_keras()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "effnetb0_128= [\n",
    "    0.9986,\n",
    "    0.9980,\n",
    "    0.9993,\n",
    "    0.9973,\n",
    "    1.0,\n",
    "]   score= 0.9893\n",
    "\n",
    "\n",
    "resnet50_v2_512= [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def TTA(model, test_x):\n",
    "    \n",
    "    predict= model.predict(test_x)\n",
    "    \n",
    "    test_flip= test_x.copy()\n",
    "    for i in range(len(test_flip)):\n",
    "        test_flip[i]= np.flip(test_flip[i], 0)\n",
    "    predict+= model.predict(test_flip)\n",
    "    del test_flip\n",
    "    \n",
    "    test_flip= test_x.copy()\n",
    "    for i in range(len(test_flip)):\n",
    "        test_flip[i]= np.flip(test_flip[i], 1)\n",
    "    predict+= model.predict(test_flip)\n",
    "    del test_flip\n",
    "        \n",
    "    predict= predict/3\n",
    "    \n",
    "    return np.array(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_CFG= {\n",
    "    'img_size': 128,\n",
    "    'load_model': 'train_cv_model'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= os.listdir(test_CFG['load_model'])\n",
    "model_name= [test_CFG['load_model']+'/'+name for name in model_name]\n",
    "print(model_name)\n",
    "\n",
    "test_x, __, img_name= load_data('Dataset/test_images', 'test', test_CFG['img_size'])\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, model in enumerate(model_name):\n",
    "    \n",
    "    model= load_model(model)\n",
    "    ans= TTA(model, test_x)\n",
    "    #ans= model.predict(test_x)\n",
    "    \n",
    "    if i==0:\n",
    "        pred= ans\n",
    "    else:\n",
    "        pred+= ans\n",
    "    reset_keras()\n",
    "    \n",
    "pred/= len(model_name)\n",
    "pred= pred.argmax(1)\n",
    "    \n",
    "submit= pd.read_csv('upload_sample.csv')\n",
    "for i, name in enumerate(img_name):\n",
    "    submit.loc[submit['ID']==name, 'Label']= pred[i]\n",
    "    \n",
    "submit.to_csv('submit.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_low_conf(test_x, test_y, conf):\n",
    "    \n",
    "    drop_index= []\n",
    "    for i in range(len(test_y)):\n",
    "        if np.max(test_y[i])<conf:\n",
    "            drop_index.append(i)\n",
    "            \n",
    "    test_x= np.delete(test_x, drop_index, axis=0)\n",
    "    test_y= np.delete(test_y, drop_index, axis=0)\n",
    "    \n",
    "    print('drop '+str(len(drop_index))+' samples')\n",
    "    \n",
    "    return test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_CFG= {\n",
    "    'img_size': 128,\n",
    "    'load_model': 'test_cv_model/effnetb0_128_0.9948',\n",
    "}\n",
    "\n",
    "\n",
    "model_name= os.listdir(test_CFG['load_model'])\n",
    "model_name= [test_CFG['load_model']+'/'+name for name in model_name]\n",
    "print(model_name)\n",
    "\n",
    "test_x, __, img_name= load_data('Dataset/test_images', 'test', test_CFG['img_size'])\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, model in enumerate(model_name):\n",
    "    \n",
    "    model= load_model(model)\n",
    "    #ans= TTA(model, test_x)\n",
    "    ans= model.predict(test_x)\n",
    "    \n",
    "    if i==0:\n",
    "        pred= ans\n",
    "    else:\n",
    "        pred+= ans\n",
    "    reset_keras()\n",
    "    \n",
    "pred/= len(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y= drop_low_conf(test_x, pred, conf= 0.57)\n",
    "test_y= test_y.argmax(1)\n",
    "test_y= to_categorical(test_y, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG= {\n",
    "    'model_name': 'effnet',\n",
    "    'img_size': 128,\n",
    "    'batch_size': 20,\n",
    "    'epoch': 30,\n",
    "    'n_fold': 5,\n",
    "    'lr': 3e-4,\n",
    "    'label_smooth': 0.5,\n",
    "    'load_model': ''#'test_cv_model/xcepnet_8.29',\n",
    "}\n",
    "\n",
    "\n",
    "all_x, all_y, img_name= load_data('Dataset/train_dataset_expand', 'train', CFG['img_size'])\n",
    "all_y= to_categorical(all_y, 6)\n",
    "index= list(range(len(all_x)))\n",
    "np.random.shuffle(index)\n",
    "all_x= all_x[index]\n",
    "all_y= all_y[index]\n",
    "\n",
    "print(all_x.shape)\n",
    "print(all_y.shape)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits= CFG['n_fold'], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_x, all_y= np.append(all_x, test_x, axis= 0), np.append(all_y, test_y, axis= 0)\n",
    "del test_x\n",
    "del test_y\n",
    "\n",
    "\n",
    "for cv, (train_index, vali_index) in enumerate(kf.split(all_x, all_y.argmax(1))):\n",
    "    #if cv==0 or cv==1: continue\n",
    "    print('start cv: ', cv)\n",
    "\n",
    "    train_x, train_y= all_x[train_index], all_y[train_index]\n",
    "    vali_x, vali_y= all_x[vali_index], all_y[vali_index]\n",
    "    epoch= CFG['epoch']\n",
    "    \n",
    "    \n",
    "    if CFG['load_model']!='':\n",
    "        print('load_model')\n",
    "        model= load_model(CFG['load_model']+'/'+'model_cv_'+str(cv)+'.h5')\n",
    "    else:\n",
    "        model= effnet(CFG['lr'], CFG['label_smooth'])\n",
    "        #model= resnet(CFG['lr'], CFG['label_smooth'])\n",
    "\n",
    "        \n",
    "    best_vali_acc= 0\n",
    "    for ep in range(epoch):\n",
    "\n",
    "        aug_x, aug_y= train_x.copy(), train_y.copy()\n",
    "        aug_x, aug_y= data_aug(aug_x, aug_y)\n",
    "        #aug_x, aug_y= mixup(aug_x, aug_y, 0.2)\n",
    "\n",
    "\n",
    "        y_integers = np.argmax(aug_y, axis=1)\n",
    "        class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "        d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        train_history= model.fit(x= aug_x, y= aug_y, validation_data=(vali_x, vali_y), epochs=1, \n",
    "                                 batch_size= CFG['batch_size'],\n",
    "                                 class_weight= d_class_weights,\n",
    "                                 verbose=1)\n",
    "\n",
    "\n",
    "        if train_history.history['val_accuracy'][0] > best_vali_acc:\n",
    "            best_vali_acc= train_history.history['val_accuracy'][0]\n",
    "            print('model save at vali_acc: ', train_history.history['val_accuracy'][0])\n",
    "            model.save('train_cv_model/model_cv_'+str(cv)+'.h5')\n",
    "\n",
    "        del aug_x\n",
    "        del aug_y\n",
    "        \n",
    "        if ep == epoch-15:\n",
    "            model= load_model('train_cv_model/model_cv_'+str(cv)+'.h5')\n",
    "            lr= CFG['lr']*0.1\n",
    "            print('reduce lr: ', lr)\n",
    "            K.set_value(model.optimizer.learning_rate, lr)\n",
    "        \n",
    "    reset_keras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_temp(pred, t= 0.5):\n",
    "    \n",
    "    new_y= []\n",
    "    for ans in pred:\n",
    "        temp_ans= [np.exp(p/t) for p in ans]\n",
    "        softmax= [p/np.sum(temp_ans) for p in temp_ans]\n",
    "        new_y.append(softmax)\n",
    "    \n",
    "    return np.array(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KD_CFG= {\n",
    "    'load_model': 'test_cv_model/KD_1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x, all_y, img_name= load_data('Dataset/train_dataset_expand', 'train', CFG['img_size'])\n",
    "all_y= to_categorical(all_y, 6)\n",
    "index= list(range(len(all_x)))\n",
    "np.random.shuffle(index)\n",
    "all_x= all_x[index]\n",
    "all_y= all_y[index]\n",
    "img_name= img_name[index]\n",
    "new_y= np.zeros_like(all_y)\n",
    "\n",
    "print(all_x.shape)\n",
    "print(all_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits= CFG['n_fold'], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cv, (train_index, vali_index) in enumerate(kf.split(all_x, all_y.argmax(1))):\n",
    "    print('start cv: ', cv)\n",
    "\n",
    "    vali_x, vali_y= all_x[vali_index], all_y[vali_index]\n",
    "    vali_name= img_name[vali_index]\n",
    "    \n",
    "    print('load_model')\n",
    "    model= load_model(KD_CFG['load_model']+'/'+'model_cv_'+str(cv)+'.h5')\n",
    "    \n",
    "\n",
    "    vali_pred= model.predict(vali_x)\n",
    "    fuse_label= (0.5*vali_y) + (0.5*vali_pred)\n",
    "    \n",
    "    \n",
    "    for i ,name in enumerate(vali_name):\n",
    "        indx= list(img_name).index(name)\n",
    "        new_y[indx]= fuse_label[i]\n",
    "        \n",
    "    reset_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG= {\n",
    "    'model_name': 'effnet',\n",
    "    'img_size': 128,\n",
    "    'batch_size': 20,\n",
    "    'epoch': 30,\n",
    "    'n_fold': 5,\n",
    "    'lr': 3e-4,\n",
    "    'label_smooth': 0,\n",
    "}\n",
    "\n",
    "\n",
    "all_y= softmax_temp(new_y, t= 0.4)\n",
    "for cv, (train_index, vali_index) in enumerate(kf.split(all_x, all_y.argmax(1))):\n",
    "    if cv!=0: continue\n",
    "    print('start cv: ', cv)\n",
    "\n",
    "    vali_x, vali_y= all_x[vali_index], all_y[vali_index]\n",
    "    epoch= CFG['epoch']\n",
    "    \n",
    "    \n",
    "    model= effnet(CFG['lr'], CFG['label_smooth'])\n",
    "\n",
    "        \n",
    "    best_vali_acc= 0\n",
    "    for ep in range(epoch):\n",
    "\n",
    "        aug_x, aug_y= all_x[train_index], all_y[train_index]\n",
    "        aug_x, aug_y= data_aug(aug_x, aug_y)\n",
    "        #aug_x, aug_y= mixup(aug_x, aug_y, 0.2)\n",
    "\n",
    "\n",
    "        y_integers = np.argmax(aug_y, axis=1)\n",
    "        class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "        d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        train_history= model.fit(x= aug_x, y= aug_y, validation_data=(vali_x, vali_y), epochs=1, \n",
    "                                 batch_size= CFG['batch_size'],\n",
    "                                 class_weight= d_class_weights,\n",
    "                                 verbose=1)\n",
    "\n",
    "\n",
    "        if train_history.history['val_accuracy'][0] > best_vali_acc:\n",
    "            best_vali_acc= train_history.history['val_accuracy'][0]\n",
    "            print('model save at vali_acc: ', train_history.history['val_accuracy'][0])\n",
    "            model.save('train_cv_model/model_cv_'+str(cv)+'.h5')\n",
    "\n",
    "        del aug_x\n",
    "        del aug_y\n",
    "        \n",
    "        if ep == epoch-15:\n",
    "            model= load_model('train_cv_model/model_cv_'+str(cv)+'.h5')\n",
    "            lr= CFG['lr']*0.5\n",
    "            print('reduce lr: ', lr)\n",
    "            K.set_value(model.optimizer.learning_rate, lr)\n",
    "        \n",
    "    reset_keras()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
